
# ğŸ“˜ Qwen3â€‘4Bâ€‘Instructâ€‘2507 â€” Oneâ€‘Page ëª¨ë¸ ë…¸íŠ¸

## 1. ëª¨ë¸ ê°œìš”

Qwen3â€‘4Bâ€‘Instructâ€‘2507ì€ **Qwen3â€‘4B Nonâ€‘Thinking ëª¨ë¸ì˜ ì—…ë°ì´íŠ¸ ë²„ì „**ìœ¼ë¡œ,  
ì¼ë°˜ ì§€ëŠ¥Â·ì¶”ë¡ Â·ë©€í‹°ë§êµ¬ì–¼Â·ì •ë ¬(Alignment)Â·ì—ì´ì „íŠ¸ ëŠ¥ë ¥ì„ ëŒ€í­ ê°œì„ í•œ **4Bê¸‰ ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ ëª¨ë¸**.

### ì£¼ìš” íŠ¹ì§•

*   âœ” **Nonâ€‘Thinking ì „ìš© ëª¨ë¸** (ì¦‰, `<think>` ë¸”ë¡ ì—†ìŒ / `enable_thinking=False` ë¶ˆí•„ìš”)
*   âœ” ì§€ì‹œ ë”°ë¥´ê¸°, ì„¸ê³„ ì§€ì‹, ì°½ì˜ì  ê¸€ì“°ê¸°, ìˆ˜í•™Â·ê³¼í•™Â·ì½”ë”© ì„±ëŠ¥ í¬ê²Œ í–¥ìƒ
*   âœ” 256K **ì´ˆì¥ë¬¸ ë¬¸ë§¥(Longâ€‘context) ë„¤ì´í‹°ë¸Œ ì§€ì›**
*   âœ” ë‹¤êµ­ì–´ ë²”ìœ„ í™•ì¥ ë° longâ€‘tail knowledge ê°•í™”
*   âœ” ë„êµ¬ í˜¸ì¶œ(agentic tool use) ê°•í™” â†’ Qwen-Agentì™€ ì—°ë™ ìµœì í™”

***

## 2. ëª¨ë¸ êµ¬ì¡° & config ê¸°ë°˜ ìš”ì•½

### ğŸ“Š í•µì‹¬ ìŠ¤í™

| í•­ëª©              | ê°’                       |
| --------------- | ----------------------- |
| íŒŒë¼ë¯¸í„° ìˆ˜          | 4.0B                    |
| (ì„ë² ë”© ì œì™¸)        | 3.6B                    |
| ë ˆì´ì–´ ìˆ˜           | 36                      |
| Attention (GQA) | 32 Qâ€‘heads / 8 KVâ€‘heads |
| Hidden size     | 2560                    |
| FFN size        | 9728                    |
| Context length  | **262,144 tokens**      |
| Activation      | SiLU                    |
| Norm            | RMSNorm (eps 1eâ€‘6)      |
| RoPE Î¸          | 5,000,000               |
| Vocab           | 151,936                 |
| Architecture    | `Qwen3ForCausalLM`      |

### ì•„í‚¤í…ì²˜ íŠ¹ì§•

*   **Full attention** ê¸°ë°˜
*   RoPE scaling ë¹„í™œì„±(ê¸°ë³¸ RoPE)
*   ëŒ€ê·œëª¨ Î¸(5M)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ˆì¥ë¬¸ ì„±ëŠ¥ ìµœì í™”
*   Slidingâ€‘window ì—†ìŒ (use\_sliding\_window=False)

***

## 3. ì„±ëŠ¥ ìš”ì•½(í•µì‹¬ë§Œ)

### âœ¨ Qwen3â€‘4B â†’ Qwen3â€‘4Bâ€‘Instructâ€‘2507 ì—…ê·¸ë ˆì´ë“œ í­ë°œì  í–¥ìƒ

| Benchmark           | ê¸°ì¡´ 4B | 4Bâ€‘Instructâ€‘2507 |
| ------------------- | ----: | ---------------: |
| MMLUâ€‘Pro            |  58.0 |         **69.6** |
| MMLUâ€‘Redux          |  77.3 |         **84.2** |
| GPQA                |  41.7 |         **62.0** |
| AIMEâ€‘25             |  19.1 |         **47.4** |
| HMMTâ€‘25             |  12.1 |         **31.0** |
| ZebraLogic          |  35.2 |         **80.2** |
| IFEval              |  81.2 |         **83.4** |
| Creative Writing v3 |  53.6 |         **83.5** |
| BFCLâ€‘v3             |  57.6 |         **61.9** |
| TAUâ€‘Retail          |  24.3 |         **48.7** |

â†’ **ì§€ì‹œ ìˆ˜í–‰, ì¶”ë¡ , ì°½ì˜ ê¸€ì“°ê¸°, ì—ì´ì „íŠ¸ê¹Œì§€ ëª¨ë“  ì¶•ì—ì„œ 4Bê¸‰ ìµœê³  ìˆ˜ì¤€**

***

## 4. ê¸°ë³¸ ì‚¬ìš©ë²•(Quickstart)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-4B-Instruct-2507"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

messages = [{"role": "user", "content": "Give me a short introduction to large language model."}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

inputs = tokenizer([text], return_tensors="pt").to(model.device)
output = model.generate(**inputs, max_new_tokens=16384)

print(tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True))
```

â€» `transformers >= 4.51.0` í•„ìˆ˜  
(`KeyError: 'qwen3'` ë°©ì§€)

***

## 5. ì—ì´ì „íŠ¸/íˆ´ ì‚¬ìš©(Qwen-Agent ê¶Œì¥)

```python
from qwen_agent.agents import Assistant

llm_cfg = {
    'model': 'Qwen3-4B-Instruct-2507',
    'model_server': 'http://localhost:8000/v1',
    'api_key': 'EMPTY',
}

tools = [
    {'mcpServers': {
        'time': {'command': 'uvx', 'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']},
        'fetch': {'command': 'uvx', 'args': ['mcp-server-fetch']}
    }},
    'code_interpreter',
]

bot = Assistant(llm=llm_cfg, function_list=tools)

messages = [{'role': 'user', 'content': 'Introduce the latest developments of Qwen'}]
for out in bot.run(messages=messages):
    pass
print(out)
```

Qwen-AgentëŠ”:

*   Toolâ€‘calling í…œí”Œë¦¿/íŒŒì„œ ìë™ í¬í•¨
*   MCP ê¸°ë°˜ tool êµ¬ì„± ì§€ì›  
    â†’ **ì½”ë“œ ë³µì¡ë„ í¬ê²Œ ê°ì†Œ**

***

## 6. ë°°í¬(Deployment)

### SGLang (0.4.6.post1 ì´ìƒ)

```bash
python -m sglang.launch_server \
  --model-path Qwen/Qwen3-4B-Instruct-2507 \
  --context-length 262144
```

### vLLM (0.8.5 ì´ìƒ)

```bash
vllm serve Qwen/Qwen3-4B-Instruct-2507 --max-model-len 262144
```

â¡ OOM ë°œìƒ ì‹œ context lengthë¥¼ 32K ë“±ìœ¼ë¡œ ì¶•ì†Œ

***

## 7. Best Practices (ê¶Œì¥ ì„¤ì •)

### Sampling

*   `temperature = 0.7`
*   `top_p = 0.8`
*   `top_k = 20`
*   í•„ìš” ì‹œ `presence_penalty âˆˆ [0, 2]`  
    (ë†’ì„ìˆ˜ë¡ ë°˜ë³µ ì–µì œ, ê·¸ëŸ¬ë‚˜ ì–¸ì–´ í˜¼í•© ê°€ëŠ¥)

### Output length

*   ê¶Œì¥: **16,384 tokens**

### Benchmarking ì‹œ ê¶Œì¥ í”„ë¡¬í”„íŠ¸

*   **ìˆ˜í•™** â†’ â€œPlease reason step by step, and put your final answer within \boxed{}.â€
*   **ê°ê´€ì‹** â†’ JSON êµ¬ì¡° í¬í•¨ `"answer": "C"`

***

## 8. Citation

    @misc{qwen3technicalreport,
          title={Qwen3 Technical Report},
          author={Qwen Team},
          year={2025},
          eprint={2505.09388},
          archivePrefix={arXiv},
          primaryClass={cs.CL},
          url={https://arxiv.org/abs/2505.09388},
    }

***

# ğŸ“˜ EXAONEâ€‘4.0â€‘1.2B â€” Oneâ€‘Page ëª¨ë¸ ë…¸íŠ¸

## 1. ëª¨ë¸ ê°œìš”

**EXAONE 4.0** ì‹œë¦¬ì¦ˆëŠ”

*   **Nonâ€‘reasoning ëª¨ë“œ**(ì¼ë°˜ ëŒ€í™”/ì§€ì‹œ ìˆ˜í–‰)ì™€
*   **Reasoning ëª¨ë“œ**(ë…¼ë¦¬Â·ìˆ˜í•™Â·ì¶”ë¡ )  
    ë‘ ê¸°ëŠ¥ì„ **í•˜ë‚˜ì˜ ëª¨ë¸ì— í†µí•©í•œ ìµœì´ˆì˜ EXAONE ë¼ì¸ì—…**.

**1.2B ëª¨ë¸**ì€ **ì˜¨ë””ë°”ì´ìŠ¤ìš© ê²½ëŸ‰ ëª¨ë¸**ë¡œ ê°œë°œë˜ì—ˆìœ¼ë©°  
í•œêµ­ì–´Â·ì˜ì–´Â·ìŠ¤í˜ì¸ì–´ê¹Œì§€ ìì—°ìŠ¤ëŸ½ê²Œ ì§€ì›.

### ì£¼ìš” íŠ¹ì§•

*   Reasoningê³¼ Nonâ€‘reasoning í†µí•©
*   ì—ì´ì „íŠ¸ ë„êµ¬ í˜¸ì¶œ(Agentic tool use) ì§€ì›
*   í•œêµ­ì–´ ì‹¤ìš© ì§€ì‹(ê³ ë‚œë„ í¬í•¨) ì„±ëŠ¥ ê°•í™”
*   ê¸´ ë¬¸ë§¥ ì²˜ë¦¬: **65,536 tokens**
*   ì†Œí˜• ëª¨ë¸ ëŒ€ë¹„ ìƒìœ„ê¶Œ ì„¸ê³„ ì§€ì‹/ìˆ˜í•™/ë„êµ¬ í˜¸ì¶œ ì„±ëŠ¥

***

## 2. ì•„í‚¤í…ì²˜ & êµ¬ì„±(config.json ê¸°ë°˜)

### ğŸ“Š í•µì‹¬ ìŠ¤í™

| í•­ëª©                  | ê°’                                      |
| ------------------- | -------------------------------------- |
| íŒŒë¼ë¯¸í„° ìˆ˜              | 1.07B (ì„ë² ë”© ì œì™¸)                         |
| ë ˆì´ì–´                 | 30                                     |
| Attention           | GQA (32 heads / 8 KV-heads)            |
| Hidden size         | 2048                                   |
| Intermediate size   | 4096                                   |
| Vocab size          | 102,400                                |
| Context length      | **65,536**                             |
| Positional Encoding | RoPE (Llama3 style, scaling factor 16) |
| Activation          | SiLU                                   |
| Normalization       | RMSNorm + QK-Reorder-Norm              |
| Dtype               | bfloat16                               |
| Architecture        | `Exaone4ForCausalLM`                   |

### ğŸ”§ êµ¬ì¡°ì  ë³€í™”(4.0ì˜ í•µì‹¬ ì°¨ë³„ì )

*   **QKâ€‘Reorderâ€‘Norm**: Q/K projection ì§í›„ RMSNorm ì ìš© â†’ ì¶”ë¡ Â·ì§€ì‹œ ì„±ëŠ¥ í–¥ìƒ
*   **Full Attention(1.2B)**: ì†Œí˜• ëª¨ë¸ íŠ¹ì„±ìƒ hybrid ëŒ€ì‹  Full attention ì‚¬ìš©
*   RoPE scalingìœ¼ë¡œ ì´ˆì¥ë¬¸ ì²˜ë¦¬ ëŠ¥ë ¥ í™•ë³´

***

## 3. ì‚¬ìš© ê°€ì´ë“œ (Quickstart)

### 3.1 ì¼ë°˜ ëª¨ë“œ(Nonâ€‘reasoning)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "LGAI-EXAONE/EXAONE-4.0-1.2B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="bfloat16",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

messages = [{"role": "user", "content": "Explain how wonderful you are"}]
input_ids = tokenizer.apply_chat_template(
    messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
)

output = model.generate(
    input_ids.to(model.device),
    max_new_tokens=128,
    do_sample=False
)
print(tokenizer.decode(output[0]))
```

***

### 3.2 Reasoning ëª¨ë“œ

`enable_thinking=True` â†’ `<think>` ë¸”ë¡ì„ ì—´ê³  Reasoning í™œì„±í™”

```python
messages = [{"role": "user", "content": "Which one is bigger, 3.12 vs 3.9?"}]
input_ids = tokenizer.apply_chat_template(
    messages, tokenize=True, add_generation_prompt=True,
    return_tensors="pt", enable_thinking=True
)

output = model.generate(
    input_ids.to(model.device),
    max_new_tokens=128,
    do_sample=True, temperature=0.6, top_p=0.95
)
print(tokenizer.decode(output[0]))
```

â€» Reasoning ëª¨ë“œëŠ” **sampling íŒŒë¼ë¯¸í„° ì˜í–¥ì´ ë§¤ìš° í¼**

***

### 3.3 Agentic Tool Use (ë„êµ¬ í˜¸ì¶œ)

```python
def roll_dice(max_num: int):
    return random.randint(1, max_num)

tools = [{
    "type": "function",
    "function": {
        "name": "roll_dice",
        "description": "Roll a dice",
        "parameters": {
            "type": "object",
            "required": ["max_num"],
            "properties": {"max_num": {"type": "int"}}
        }
    }
}]

messages = [{"role": "user", "content": "Roll D6 dice twice!"}]
input_ids = tokenizer.apply_chat_template(
    messages, tokenize=True, add_generation_prompt=True,
    return_tensors="pt", tools=tools
)

output = model.generate(
    input_ids.to(model.device),
    max_new_tokens=1024,
    do_sample=True, temperature=0.6, top_p=0.95
)
print(tokenizer.decode(output[0]))
```

***

## 4. ë°°í¬(Deployment)

### TensorRTâ€‘LLM

```bash
git clone https://github.com/NVIDIA/TensorRT-LLM.git
```

ì¶”ê°€ ì„¤ì • íŒŒì¼ ì˜ˆì‹œ:

```yaml
# extra_llm_api_config.yaml
kv_cache_config:
  enable_block_reuse: false
```

ì„œë²„ ì‹¤í–‰:

```bash
trtllm-serve serve LGAI-EXAONE/EXAONE-4.0-1.2B \
  --backend pytorch \
  --extra_llm_api_options extra_llm_api_config.yaml
```

***

### vLLM (0.10.0 ì´ìƒ)

```bash
vllm serve LGAI-EXAONE/EXAONE-4.0-1.2B \
  --enable-auto-tool-choice \
  --tool-call-parser hermes \
  --reasoning-parser deepseek_r1
```

***

## 5. ì„±ëŠ¥ ìš”ì•½(1.2B ê¸°ì¤€ í•µì‹¬ë§Œ)

### Reasoning Mode (1.2B)

*   **MMLUâ€‘Redux**: 71.5
*   **AIMEâ€‘2025**: 45.2 (ì†Œí˜• ëª¨ë¸ ì¤‘ ìš°ìˆ˜)
*   **BFCLâ€‘v3(Tool use)**: 52.9
*   **KMMLUâ€‘Redux(í•œêµ­ì–´)**: 46.9
*   **MMMLU(ES)**: 62.4

### Nonâ€‘Reasoning Mode (1.2B)

*   **MMLUâ€‘Redux**: 66.9
*   **IFEval**: 74.7 (ì§€ì‹œ ìˆ˜í–‰ ë§¤ìš° ê°•í•¨)
*   **Long context (RULER)**: 77.4
*   **Koâ€‘LongBench**: 69.8

ğŸ‘‰ **ê²½ëŸ‰ ëª¨ë¸ ì¤‘ ê°€ì¥ ê· í˜• ì¡íŒ ì„±ëŠ¥(ì§€ì‹Â·ì¶”ë¡ Â·í•œêµ­ì–´Â·ìŠ¤í˜ì¸ì–´Â·ë„êµ¬ ì‚¬ìš©)**

***

## 6. ì¶”ì²œ ì‚¬ìš© ì„¤ì • (Usage Guideline)

| ëª¨ë“œ              | ê¶Œì¥ ì„¤ì •                                 |
| --------------- | ------------------------------------- |
| Nonâ€‘reasoning   | `temperature < 0.6`                   |
| Reasoning       | `temperature=0.6`, `top_p=0.95`       |
| Degeneration ë°©ì§€ | `presence_penalty=1.5`                |
| í•œêµ­ì–´ ì¼ë°˜ ëŒ€í™”(1.2B) | `temperature=0.1` (code-switching ë°©ì§€) |

***

## 7. ë¼ì´ì„ ìŠ¤ ìš”ì•½

**EXAONE AI Model License Agreement 1.2 â€“ NC**

*   ì¶œë ¥ë¬¼ ì†Œìœ ê¶Œ ì œí•œ ì¡°í•­ ì‚­ì œë¨
*   **ê²½ìŸ ëª¨ë¸ ê°œë°œìš© ì‚¬ìš© ê¸ˆì§€**
*   ì—°êµ¬ + êµìœ¡ ëª©ì  í—ˆìš©

***

## 8. ì°¸ê³ 

Citation:

    @article{exaone-4.0,
      title={EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes},
      author={{LG AI Research}},
      journal={arXiv preprint arXiv:2507.11407},
      year={2025}
    }

***

# ğŸ“˜ Qwen3â€‘1.7B â€” Oneâ€‘Page ëª¨ë¸ ë…¸íŠ¸

## 1. ëª¨ë¸ ê°œìš”

**Qwen3â€‘1.7B**ëŠ” Qwen3 ì„¸ëŒ€ì˜ **ê²½ëŸ‰ ì¤‘í˜• ëª¨ë¸**ë¡œ,  
ë‹¤ìŒ íŠ¹ì§•ì´ í•µì‹¬ì…ë‹ˆë‹¤:

### ì£¼ìš” ê¸°ëŠ¥ ìš”ì•½

*   âœ” **Thinking â†” Nonâ€‘Thinking ëª¨ë“œ ì™„ì „ í†µí•©**  
    â†’ í•˜ë‚˜ì˜ ëª¨ë¸ì—ì„œ ë…¼ë¦¬ ì¶”ë¡ /ìˆ˜í•™/ì½”ë”© ê°•í™” ëª¨ë“œì™€ ë¹ ë¥¸ ì¼ë°˜ ëŒ€í™” ëª¨ë“œë¥¼ ì „í™˜ ê°€ëŠ¥
*   âœ” QwQÂ·Qwen2.5 ëŒ€ë¹„ **ëŒ€í­ í–¥ìƒëœ ì¶”ë¡  ëŠ¥ë ¥**
*   âœ” í’ë¶€í•œ ì¸ê°„ ì„ í˜¸ ì •ë ¬(Alignment)  
    â†’ ì°½ì‘, ë¡¤í”Œë ˆì‰, ë©€í‹°í„´ ëŒ€í™” ë“± ìì—°ìŠ¤ëŸ¬ìš´ ìƒí˜¸ì‘ìš©
*   âœ” **Agent(ë„êµ¬ í˜¸ì¶œ) ê°•í™”**  
    â†’ reasoning/ë¹„â€‘reasoning ëª¨ë‘ì—ì„œ ë„êµ¬ í˜¸ì¶œ ë™ì‘
*   âœ” 100+ ì–¸ì–´ ì§€ì› (ë²ˆì—­ ë° ë‹¤êµ­ì–´ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ì„±ëŠ¥ ê°•í™”)

***

## 2. ëª¨ë¸ êµ¬ì¡° (config ê¸°ë°˜)

### ğŸ“Š í•µì‹¬ ìŠ¤í™

| í•­ëª©                | ê°’                       |
| ----------------- | ----------------------- |
| íŒŒë¼ë¯¸í„°              | 1.7B                    |
| (ì„ë² ë”© ì œì™¸)          | 1.4B                    |
| ë ˆì´ì–´               | 28                      |
| Attention (GQA)   | 16 Q-heads / 8 KV-heads |
| Hidden size       | 2048                    |
| FFN(intermediate) | 6144                    |
| Context length    | **32,768**              |
| RoPE Î¸            | 1,000,000               |
| Activation        | SiLU                    |
| Norm              | RMSNorm (eps=1eâ€‘6)      |
| Architecture      | `Qwen3ForCausalLM`      |
| Vocab             | 151,936                 |
| Dtype             | bfloat16                |
| Sliding-window    | ì—†ìŒ                      |

***

## 3. Thinking/Nonâ€‘Thinking ëª¨ë“œ

### enable\_thinking=True (ê¸°ë³¸)

*   `<think> ... </think>` ë¸”ë¡ ìƒì„±
*   math/logic/codeì—ì„œ ìµœì  ì„±ëŠ¥
*   ê¶Œì¥ Sampling:
    *   `temperature=0.6`, `top_p=0.95`, `top_k=20`

### enable\_thinking=False

*   `<think>` ë¸”ë¡ ì™„ì „ ë¹„í™œì„±
*   ë¹ ë¥¸ inference/ì¼ë°˜ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ìµœì 
*   ê¶Œì¥ Sampling:
    *   `temperature=0.7`, `top_p=0.8`, `top_k=20`

### Soft switch (ìœ ì € ì…ë ¥ ê¸°ë°˜)

*   `/think` â†’ í•´ë‹¹ í„´ë¶€í„° Thinking
*   `/no_think` â†’ í•´ë‹¹ í„´ Nonâ€‘Thinking
*   enable\_thinking=Trueì¼ ë•Œë§Œ ë™ì‘
*   enable\_thinking=Falseë©´ soft switch ë¬´ì‹œë¨

***

## 4. Quickstart (Thinking ëª¨ë“œ ì˜ˆì‹œ)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-1.7B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name, torch_dtype="auto", device_map="auto"
)

messages = [{"role": "user", "content": "Give me a short introduction to large language model."}]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True  # ê¸°ë³¸ê°’
)

inputs = tokenizer([text], return_tensors="pt").to(model.device)
gen = model.generate(**inputs, max_new_tokens=32768)[0]

# thinking/answer ë¶„ë¦¬
out = gen[len(inputs.input_ids[0]):].tolist()
try:
    idx = len(out) - out[::-1].index(151668)  # </think>
except ValueError:
    idx = 0

thinking = tokenizer.decode(out[:idx], skip_special_tokens=True).strip()
answer   = tokenizer.decode(out[idx:], skip_special_tokens=True).strip()

print("thinking:", thinking)
print("answer:", answer)
```

***

## 5. Thinking â†” Nonâ€‘Thinking ì „í™˜ ì˜ˆì‹œ

### Nonâ€‘Thinking ëª¨ë“œ

```python
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=False
)
```

### Soft switching ì˜ˆì‹œ

    User: How many r's in strawberries?
    User: Then how many r's in blueberries? /no_think
    User: Really? /think

***

## 6. Agentic Tool Use (Qwen-Agent)

```python
from qwen_agent.agents import Assistant

llm_cfg = {
    'model': 'Qwen3-1.7B',
    'model_server': 'http://localhost:8000/v1',
    'api_key': 'EMPTY'
}

tools = [
    {'mcpServers': {
        'time': {'command': 'uvx', 'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']},
        'fetch': {'command': 'uvx', 'args': ['mcp-server-fetch']}
    }},
    'code_interpreter'
]

bot = Assistant(llm=llm_cfg, function_list=tools)

for res in bot.run(messages=[{"role":"user","content":"Introduce Qwen's latest progress"}]):
    pass
print(res)
```

***

## 7. ë°°í¬(Deployment)

### SGLang

```bash
python -m sglang.launch_server \
  --model-path Qwen/Qwen3-1.7B \
  --reasoning-parser qwen3
```

### vLLM

```bash
vllm serve Qwen/Qwen3-1.7B \
  --enable-reasoning \
  --reasoning-parser deepseek_r1
```

***

## 8. Best Practices

### Thinking ëª¨ë“œ

*   `temperature=0.6`, `top_p=0.95`, `top_k=20`, `min_p=0`
*   âŒ Greedy decoding ê¸ˆì§€ (ì„±ëŠ¥ ì €í•˜Â·ë°˜ë³µ ë°œìƒ)

### Nonâ€‘Thinking ëª¨ë“œ

*   `temperature=0.7`, `top_p=0.8`, `top_k=20`

### ë°˜ë³µ ì–µì œ

*   `presence_penalty âˆˆ [0, 2]`  
    (ë†’ì´ë©´ ì–¸ì–´ í˜¼í•© ê°€ëŠ¥)

### ì¶œë ¥ ê¸¸ì´

*   ê¶Œì¥: **32,768 tokens**
*   ê³ ë‚œë„ math/coding ë²¤ì¹˜ë§ˆí¬: **38,912 tokens**

### ë©€í‹°í„´ ì‹œ Best practice

*   íˆìŠ¤í† ë¦¬ì—ëŠ” **ìµœì¢… ë‹µë³€ë§Œ** ì €ì¥
*   `<think>` ë‚´ìš©ì€ íˆìŠ¤í† ë¦¬ì— í¬í•¨ X

***

## 9. Citation

    @misc{qwen3technicalreport,
          title={Qwen3 Technical Report},
          author={Qwen Team},
          year={2025},
          eprint={2505.09388},
          archivePrefix={arXiv},
          primaryClass={cs.CL},
          url={https://arxiv.org/abs/2505.09388},
    }

***

# ğŸ“˜ EXAONEâ€‘3.5â€‘2.4Bâ€‘Instruct â€” Oneâ€‘Page ëª¨ë¸ ë…¸íŠ¸

## 1. ëª¨ë¸ ê°œìš”

\*\*EXAONE 3.5(2.4B)\*\*ëŠ” LG AI Researchê°€ ê³µê°œí•œ **ì˜Â·í•œ ì´ì¤‘ì–¸ì–´(English/Korean) ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ LLM**ìœ¼ë¡œ,  
ë‹¤ìŒ íŠ¹ì„±ì„ ê°–ëŠ” **ì†Œí˜•Â·ìƒìš© ë°°í¬ ìµœì í™” ëª¨ë¸**ì…ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

*   âœ” **2.4B ê²½ëŸ‰ ëª¨ë¸** â€” ì‘ì€ GPU/ì˜¨ë””ë°”ì´ìŠ¤ ë°©í–¥
*   âœ” **32K í† í°** ì¥ë¬¸ ì§€ì›
*   âœ” ì˜ì–´+í•œêµ­ì–´ ìì—°ìŠ¤ëŸ¬ìš´ ì´ì¤‘ì–¸ì–´ ëª¨ë¸
*   âœ” ì‹¤ì‚¬ìš© ì¤‘ì‹¬ ì„±ëŠ¥ ìµœì í™” (MT-Bench, LiveBench, KoMT-Bench ë“± ìš°ìˆ˜)
*   âœ” Word embedding tied (7.8B/32BëŠ” untied)
*   âœ” ì‹¤ì œ ì„œë¹„ìŠ¤/ì±—ë´‡ì— ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ ì•ˆì •ì  ì¸ìŠ¤íŠ¸ëŸ­ì…˜ íŠœë‹

***

## 2. ëª¨ë¸ êµ¬ì¡°(config.json ê¸°ë°˜)

### ğŸ“Š í•µì‹¬ ìŠ¤í™

| í•­ëª©                  | ê°’                             |
| ------------------- | ----------------------------- |
| íŒŒë¼ë¯¸í„°(Nonâ€‘Embedding) | **2.14B**                     |
| ë ˆì´ì–´ ìˆ˜               | 30                            |
| Hidden size         | 2560                          |
| FFN(intermediate)   | 7168                          |
| Attention           | GQA (32 Qâ€‘heads / 8 KVâ€‘heads) |
| Head dim            | 80                            |
| Context length      | **32,768**                    |
| Activation          | SiLU                          |
| Positional encoding | RoPE (Llama3 style, factor=8) |
| Norm                | LayerNorm (eps 1eâ€‘5)          |
| Vocab size          | 102,400                       |
| Embedding tied      | **True**                      |
| Architecture        | `ExaoneForCausalLM`           |

â†’ **ë©”ëª¨ë¦¬ ëŒ€ë¹„ ì„±ëŠ¥ íš¨ìœ¨ì´ ë§¤ìš° ì¢‹ë„ë¡ ì„¤ê³„ëœ êµ¬ì¡°**

***

## 3. Quickstart (ê³µì‹ ì˜ˆì œ ìš”ì•½)

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

messages = [
    {"role": "system", "content": "You are EXAONE model from LG AI Research, a helpful assistant."},
    {"role": "user", "content": "ìŠ¤ìŠ¤ë¡œë¥¼ ìë‘í•´ ë´"}
]

input_ids = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt"
)

output = model.generate(
    input_ids.to(model.device),
    eos_token_id=tokenizer.eos_token_id,
    max_new_tokens=128,
    do_sample=False
)
print(tokenizer.decode(output[0]))
```

### ì¤‘ìš”!

*   **EXAONE 3.5ëŠ” system prompt ìµœì í™” í•™ìŠµë¨ â†’ ë°˜ë“œì‹œ system role ì‚¬ìš© ê¶Œì¥**

***

## 4. ì„±ëŠ¥ ìš”ì•½(ëŒ€í‘œ ì§€í‘œ)

| ëª¨ë¸                  | MTâ€‘Bench | LiveBench | Arenaâ€‘Hard | AlpacaEval |   IFEval | KoMTâ€‘Bench | LogicKor |
| ------------------- | -------: | --------: | ---------: | ---------: | -------: | ---------: | -------: |
| **EXAONE 3.5 2.4B** | **7.81** |  **33.0** |   **48.2** |   **37.1** | **73.6** |   **7.24** | **8.51** |
| Qwen2.5 3B          |     7.21 |      25.7 |       26.4 |       17.4 |     60.8 |       5.68 |     5.21 |
| Qwen2.5 1.5B        |     5.72 |      19.2 |       10.6 |        8.4 |     40.7 |       3.87 |     3.60 |
| Llama 3.2 3B        |     6.94 |      24.0 |       14.2 |       18.7 |     70.1 |       3.16 |     2.86 |
| Gemma2 2B           |     7.20 |      20.0 |       19.1 |       29.1 |     50.5 |       4.83 |     5.29 |

â†’ **íŠ¹íˆ í•œêµ­ì–´ ì‹¤ì‚¬ìš© ì§€í‘œ(KoMTâ€‘Bench, LogicKor)ì—ì„œ ë™ê¸‰ ìµœê³  ì„±ëŠ¥**

***

## 5. ë°°í¬(Deployment)

ì§€ì›ë˜ëŠ” í”„ë ˆì„ì›Œí¬:

*   **TensorRTâ€‘LLM**
*   **vLLM**
*   **SGLang**
*   **llama.cpp**
*   **Ollama**

â†’ ì†Œí˜• ëª¨ë¸ íŠ¹ì„±ìƒ **vLLM / SGLang / GGUF(quant)** ì¡°í•©ì´ ê°€ì¥ ì‹¤ì „ ìµœì í™”

***

## 6. Quantization(ì–‘ìí™”)

LG AI Researchì—ì„œ **AWQ / GGUF** ì–‘ìí™” ëª¨ë¸ ì œê³µ

*   2/3/4-bit ë“± ë‹¤ì–‘í•œ ì–‘ìí™” ì˜µì…˜
*   cpu/onâ€‘device í™˜ê²½ì—ì„œë„ ì‹¤ì‚¬ìš© ê°€ëŠ¥

â†’ â€œEXAONE 3.5 collectionâ€ í˜ì´ì§€ ì°¸ê³ 

***

## 7. ëª¨ë¸ ì‚¬ìš© íŒ (Best Practices)

*   **system prompt ë°˜ë“œì‹œ í¬í•¨**  
    â†’ í•™ìŠµ ê³¼ì •ì—ì„œ system role ì •ë³´ë¥¼ ì ê·¹ ë°˜ì˜í•¨
*   do\_sample=False ì‹œ ë§¤ìš° ì•ˆì •ì ì¸ ì¶œë ¥
*   í•œêµ­ì–´ ëŒ€í™”ì—ì„œ ë†’ì€ ì¼ê´€ì„±
*   32K contextë¥¼ í™œìš©í•´ ë¬¸ì„œ ìš”ì•½, RAG ê¸°ë°˜ ì¶”ë¡  ë“±ì— ì í•©

***

## 8. ì œí•œ ì‚¬í•­

*   ìµœì‹  ì •ë³´ ë°˜ì˜ X â†’ í˜„ì‹¤ ì„¸ê³„ ìµœì‹  ë°ì´í„°ëŠ” í‹€ë¦´ ìˆ˜ ìˆìŒ
*   í•™ìŠµ ë°ì´í„° ê¸°ë°˜ í¸í–¥ ì¡´ì¬ ê°€ëŠ¥
*   ì˜ëª»ëœ ë¬¸ì¥ ìƒì„±, ë¶ˆì™„ì „í•œ ì¶”ë¡  ê°€ëŠ¥
*   ë¯¼ê°í•œ ë‚´ìš©ì— ëŒ€í•´ ë¶€ì ì ˆ ì‘ë‹µ ê°€ëŠ¥ì„± â†’ ì‚¬ìš©ì ê²€ì¦ í•„ìš”

***

## 9. ë¼ì´ì„ ìŠ¤

**EXAONE AI Model License Agreement 1.1 â€” NC**

*   ë¹„ìƒì—…ì  ì‚¬ìš© ì¤‘ì‹¬
*   ì„¸ë¶€ ë‚´ìš©ì€ ë ˆí¬ì§€í† ë¦¬ License ì°¸ê³ 

***

## 10. Citation

    @article{exaone-3.5,
      title={EXAONE 3.5: Series of Large Language Models for Real-world Use Cases},
      author={LG AI Research},
      journal={arXiv preprint arXiv:2412.04862},
      year={2024}
    }

***
